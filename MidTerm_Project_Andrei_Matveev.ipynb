{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nervous-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-advertising",
   "metadata": {},
   "source": [
    "# Midterm Project-Building a Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-consumer",
   "metadata": {},
   "source": [
    "The purpose of this project is to train a classifier to detect email spams (spam $y=1$, non-spam $y=0$).\n",
    "\n",
    "First, we need to change any text email to numerical values (a feature vector $\\boldsymbol{x}\\in\\mathbb{R}^n$). We walk through the steps of constructing such a vector from an email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-argentina",
   "metadata": {},
   "source": [
    "## Preprocessing Emails (20 pts)\n",
    "\n",
    "The file named \"email1.txt\" contains an example email that contains words, numbers, an email address, a URL, special letters (e.g. the dollar sign). Since many emails may contain numbers, email addresses, URLs, special letters, it is common practice to \"normalize\" these values, so that all email addresses, URLs, etc. are treated the same. For example, we can replace any URL with a special string \"httpaddress\" to indicate a URL exists. The purpose is to let the spam classifier know a URL is present, instead of what the URL is. Following this idea, we will implement the following steps for any raw email:\n",
    "\n",
    "1. Lower-casing: convert all letters to lower case\n",
    "2. Normalizing URLs: All URLs are replaced with the text \"httpaddr\".\n",
    "3. Normalizing email addresses: Replace all email addresses with the text \"emailaddr\"\n",
    "4. Normalizing numbers: Replace all numbers with the text \"number\".\n",
    "5. Normalizing Dollars: Replace all dollar signs $\\$$ with the text \"dollar\".\n",
    "6. Stemming words: For example, \"replacing\", \"replaces\", \"replaced\" are all replaced with \"replace\"\n",
    "7. Removing non-words: Remove non-words and punctuations. All white spaces (tabs, newlines ('\\n'), spaces) should be trimmed to a single space character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-austin",
   "metadata": {},
   "source": [
    "To achieve all the 7 steps, we look at an example. Read in \"email1.txt\" as a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frozen-inclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"> Anyone knows how much it costs to host a web portal ?\\n>\\nWell, it depends on how many visitors you're expecting.\\nThis can be anywhere from less than 10 bucks a month to a couple of $100. \\nYou should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \\nif youre running something big..\\n\\nTo unsubscribe yourself from this mailing list, send an email to:\\ngroupname-unsubscribe@egroups.com\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = open('email1.txt', 'r').read()\n",
    "email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-strike",
   "metadata": {},
   "source": [
    "Here are the codes to realize all the steps above in Python. Suppose we have a string variable named str1.\n",
    "\n",
    "1. str1.lower() converts all uppercase characters in str1 into lowercase characters (note that str1 itself does not change.)\n",
    "2. Replace URLs by: re.sub(r'(http|https)://[^\\s]*', 'httpaddr', str1)\n",
    "3. Replace email addresses by: re.sub('[^\\s]+@[^\\s]+', 'emailaddr', str1);\n",
    "4. Replace all numbers by: re.sub('[0-9]+', 'number', str1)\n",
    "5. Replace dollar sign by: re.sub('[$]+', 'dollar', str1)\n",
    "6. Word Stemming: first build a PorterStemmer class by: ps = PorterStemmer(), then stemming the words by: \n",
    "\n",
    "str1 = \" \".join([ps.stem(word) for word in str1.split()])\n",
    "\n",
    "7. Remove non-words and punctuations by: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "downtown-affect",
   "metadata": {},
   "source": [
    "re.sub(r'[\\'\\\"@$%/#.:,&*+=?!\\[\\](){}>_<]', '', str1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-shape",
   "metadata": {},
   "source": [
    "Trim white spaces to a single space character by: re.sub(r'\\s+',' ', str1)\n",
    "\n",
    "Finally strip the string (remove spaces at the beginning and at the end of the string) by:\n",
    "str1 = str1.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-denial",
   "metadata": {},
   "source": [
    "Try each of this steps on the email string, and see if it works (you don't need to show the process). If everything works, then put everything in a function of the following form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satisfactory-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessEmail(emailstring):\n",
    "    \"\"\"\n",
    "    Process the 7 steps above for an email string\n",
    "    input: emailstring: the string that contains the email content. type: string\n",
    "    return: a string that is the processed email content\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-midwest",
   "metadata": {},
   "source": [
    "Finish the body of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-worse",
   "metadata": {},
   "source": [
    "Run the function ProcessEmail for the email string that was read in earlier. If you get the following content, then congratulations, your code works.\n",
    "\n",
    "anyon know how much it cost to host a web portal well it depend on how mani visitor your expect thi can be anywher from less than number buck a month to a coupl of dollarnumb you should checkout httpaddr or perhap amazon ecnumb if your run someth big to unsubscrib yourself from thi mail list send an email to emailaddr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-greenhouse",
   "metadata": {},
   "source": [
    "## Vocabulary List (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-possibility",
   "metadata": {},
   "source": [
    "After the emails are preprocessed, we now have a string that contains a list of words for\n",
    "each email. In the next step, we choose the words we would like to use in our classifier and ignore the rest. Suppose, based on a large number of email samples, we have already chosen the most frequently occuring words as the set of words to be considered, known as the vocabulary list, which is stored in vocab.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>abil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>abl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>abov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>abus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>ac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>accept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     word\n",
       "0      1       aa\n",
       "1      2       ab\n",
       "2      3     abil\n",
       "3      4      abl\n",
       "4      5    about\n",
       "5      6     abov\n",
       "6      7  absolut\n",
       "7      8     abus\n",
       "8      9       ac\n",
       "9     10   accept"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.read_csv('vocab.txt', delim_whitespace=True, names=['index', 'word'])\n",
    "vocab.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-airport",
   "metadata": {},
   "source": [
    "Each word has an index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-remedy",
   "metadata": {},
   "source": [
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of indices, each of which is the index of the word in the vocabulary list. Specifically, in the sample email, the word \"anyon\" can be mapped onto the index 86 in the vocabulary\n",
    "list. If a word is not in the vocabulary, ignore it. Now write a function in the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accomplished-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmailToIndices(processed_email, vocab_df):\n",
    "    \"\"\"\n",
    "    Turn processed email string to a list of indices\n",
    "    input: processed_email: a string of preprocessed email\n",
    "           vocab_df: the vocabulary data frame\n",
    "    return: a list of integers, which are the indices of the words in the vocabulary list\n",
    "    Hint1: First, you need to turn the email string to a list of words by \n",
    "           processed_email.split(); then for each word check if it is in the vocabulary;\n",
    "           if it is in it, then find the corresponding index and append it to your \n",
    "           index list; otherwise ignore it.\n",
    "    Hint2: To build the list of indices, you can start with an empty list [] and append\n",
    "           indices one by one as you find them using the append function associated with \n",
    "           the list\n",
    "    Hint3: to check if a word, say \"anyon\" appears in the \"word\" column of the vocabulary\n",
    "           you can use \"anyon\" in vocab['word'].values, which will give you a True or\n",
    "           False\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-rhythm",
   "metadata": {},
   "source": [
    "Test your EmailToIndices function on the preprocessed email string obtained earlier. If you get the following list of integers, congratulations again, your code works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cocktail",
   "metadata": {},
   "source": [
    "[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831, 883, 431, 1171, 794, 1002, 1895, 592, 1676, 238, 162, 89, 688, 945, 1663, 1120, 1062, 1699, 375, 1162, 479, 1893, 1510, 799, 1182, 1237, 810, 1895, 1440, 1547, 181, 1699, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699, 531]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-catering",
   "metadata": {},
   "source": [
    "## Extracting Features from Emails (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-integration",
   "metadata": {},
   "source": [
    "Now we will convert each email into a vector in $R^n$ as the features, where $n$ is the number of words in the vocabulary list. Specifically, the feature $x_i=0$ or $1$ for an email depending on whether the $i$-th word in the vocabulary list occurs in the email. That is, $x_i = 1$ if the $i$-th word is in the email and $x_i = 0$ if the i-th word is not present in the email. Write a function that converts a list of indices obtained from the previous function to a feature vector in $R^n$. Follow the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "included-validity",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def FeatureExtraction(indices, vocab_df):\n",
    "    \"\"\"\n",
    "    Convert a list of word indices to a feature vector\n",
    "    input: indices: a list of integer, which are the indices of the words in the email\n",
    "           vocab_df: the vocabulary data frame\n",
    "    return: a one-dimensional numpy array that contains the features. The size of the\n",
    "            array is equal to the number of rows of the vocabulary data frame\n",
    "    PAY ATTENTION: the index in the vocabulary starts with 1, but for numpy array index\n",
    "                   starts from 0\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-posting",
   "metadata": {},
   "source": [
    "Run the function on the list of indices obtained from the previous problem to get the feature vector. Report the number of nonzeros in the vector and the size of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-strength",
   "metadata": {},
   "source": [
    "## Training SVM for Spam Classification (30 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-medium",
   "metadata": {},
   "source": [
    "Now suppose we have preprocessed a bunch of emails and manually labled them. Each original email was processed using the ProcessEmail, EmailToIndices, and FeatureExtraction functions in sequence, and converted into a vector in $R^{1899}$. We use the data to train a Linear SVM classifier. The file \"spamTrain.npy\" contains 4000 training examples of spam and non-spam email, while \"spamTest.npy\" contains 1000 test examples. The last column is the target label (spam: y=1, non-spam: y=0). Use np.load(filename) to read in the data.\n",
    "\n",
    "1. For training the linear SVM, consider $C=\\{0.05, 1.0, 2.0\\}$. Set random_state=10, loss='hinge', and all the other parameters are by default. For each model report the accuracy on the test data (LinearSVC has a built in function to calculate accuracy). Also report the best model based on the accuracy values.\n",
    "\n",
    "2. Look at the parameters $\\boldsymbol{w}=\\{w_1,w_2,\\dots,w_n\\}$ of the underlying linear model. Make a plot of the absolute values of $\\boldsymbol{w}$ versus the indices $[0,1,2,...n-1]$. Are some weights $w_i$ significantly larger than the rest of the weights from the graph? If so, the words corresponding to the larger weights are the most predictive of spam. Find those words. (Hint: you need to find the indices (positions) of the 15 largest values in the vector $\\boldsymbol{w}$. The function np.argsort may help. Check the usage of the function. Then use the indices to find the words in the vocabulary data frame."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
